# LLM Evaluation using DeepEval

This script is designed to evaluate large language models (LLMs) using the DeepEval framework. Follow the steps below to set up and run the evaluation.

## Prerequisites

Ensure you have Python installed on your system. You can download it from [python.org](https://www.python.org/).

## Installation

1. Clone this repository to your local machine.

    ```sh
    git clone <repository-url>
    cd <repository-directory>
    ```

2. Install the required packages using `requirements.txt`.

    ```sh
    pip install -r requirements.txt
    ```

## Configuration

Before running the evaluation, you can adjust the hyper-parameters in the `config.py` file to suit your needs. You can change the base model and other hyper parameters in this file.



## Running the Evaluation

Execute the evaluation.py script to start the evaluation process
```bash
python evaluation.py
```

## Evaluation Methods
For detailed information on the evaluation methods used here or to use other evaluation methods, refer to the [DeepEval documentation](https://docs.confident-ai.com/docs/getting-started).